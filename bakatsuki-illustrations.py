import os
import re
import time
import requests
import io # C·∫ßn thi·∫øt cho vi·ªác n√©n trong b·ªô nh·ªõ
from PIL import Image # Th∆∞ vi·ªán Pillow ƒë·ªÉ x·ª≠ l√Ω ·∫£nh
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# === C·∫§U H√åNH ===
PROJECT_URL = 'https://www.baka-tsuki.org/project/index.php?title=Absolute_Duo'
OUTPUT_FOLDER = 'test/results'
BASE_URL = 'https://www.baka-tsuki.org'

# C·∫•u h√¨nh Selenium
chrome_options = Options()
chrome_options.add_argument("--headless=new")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--window-size=1920,1080")
chrome_options.add_argument("--log-level=3")
chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])

try:
    driver = webdriver.Chrome(options=chrome_options)
except WebDriverException as e:
    print(f"L·ªói kh·ªüi t·∫°o WebDriver: {e}")
    print("Vui l√≤ng ƒë·∫£m b·∫£o b·∫°n ƒë√£ c√†i ƒë·∫∑t Chrome v√† ChromeDriver phi√™n b·∫£n t∆∞∆°ng th√≠ch.")
    exit()

def compress_and_save_image(image_path, save_path, target_mb=1.0):
    """
    N√©n m·ªôt ·∫£nh ƒë·ªÉ c√≥ k√≠ch th∆∞·ªõc d∆∞·ªõi target_mb v√† l∆∞u l·∫°i.
    """
    target_bytes = target_mb * 1024 * 1024
    
    # N·∫øu ·∫£nh g·ªëc ƒë√£ ƒë·ªß nh·ªè, ch·ªâ c·∫ßn sao ch√©p n√≥
    if os.path.getsize(image_path) <= target_bytes:
        print(f"      ‚Üí ·∫¢nh g·ªëc ƒë√£ nh·ªè h∆°n {target_mb}MB. Sao ch√©p tr·ª±c ti·∫øp.")
        os.rename(image_path, save_path)
        return

    try:
        with Image.open(image_path) as img:
            # Chuy·ªÉn ƒë·ªïi sang RGB n·∫øu l√† RGBA ho·∫∑c P (PNG) ƒë·ªÉ l∆∞u d∆∞·ªõi d·∫°ng JPEG
            if img.mode in ('RGBA', 'P'):
                img = img.convert('RGB')
            
            # Th·ª≠ gi·∫£m ch·∫•t l∆∞·ª£ng tr∆∞·ªõc
            for quality in range(85, 10, -5):
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG', quality=quality)
                if buffer.tell() <= target_bytes:
                    print(f"      ‚Üí ƒê√£ n√©n ·∫£nh th√†nh c√¥ng (ch·∫•t l∆∞·ª£ng: {quality}%)")
                    with open(save_path, 'wb') as f:
                        f.write(buffer.getvalue())
                    return
            
            # N·∫øu gi·∫£m ch·∫•t l∆∞·ª£ng kh√¥ng ƒë·ªß, h√£y gi·∫£m k√≠ch th∆∞·ªõc ·∫£nh
            print("      ‚Üí Gi·∫£m ch·∫•t l∆∞·ª£ng kh√¥ng ƒë·ªß, ti·∫øn h√†nh gi·∫£m ƒë·ªô ph√¢n gi·∫£i...")
            w, h = img.size
            img.thumbnail((w * 0.9, h * 0.9)) # Gi·∫£m 10% m·ªói l·∫ßn
            
            # Th·ª≠ l·∫°i v·ªõi ch·∫•t l∆∞·ª£ng cao sau khi ƒë√£ gi·∫£m k√≠ch th∆∞·ªõc
            for quality in range(95, 10, -5):
                buffer = io.BytesIO()
                img.save(buffer, format='JPEG', quality=quality)
                if buffer.tell() <= target_bytes:
                    print(f"      ‚Üí ƒê√£ n√©n ·∫£nh th√†nh c√¥ng (gi·∫£m res, ch·∫•t l∆∞·ª£ng: {quality}%)")
                    with open(save_path, 'wb') as f:
                        f.write(buffer.getvalue())
                    return
            
            print(f"      ‚úó Kh√¥ng th·ªÉ n√©n ·∫£nh xu·ªëng d∆∞·ªõi {target_mb}MB. L∆∞u phi√™n b·∫£n cu·ªëi c√πng.")
            img.save(save_path, format='JPEG', quality=10)

    except Exception as e:
        print(f"      ‚úó L·ªói khi n√©n ·∫£nh: {e}")
        # N·∫øu l·ªói, ch·ªâ c·∫ßn di chuy·ªÉn file g·ªëc
        os.rename(image_path, save_path)
    finally:
        # D·ªçn d·∫πp file t·∫°m n·∫øu c√≤n t·ªìn t·∫°i
        if os.path.exists(image_path):
            os.remove(image_path)

def download_image_and_compress(url, folder, filename):
    """T·∫£i m·ªôt file ·∫£nh, sau ƒë√≥ n√©n v√† l∆∞u l·∫°i."""
    os.makedirs(folder, exist_ok=True)
    base_filename, _ = os.path.splitext(filename)
    final_save_path = os.path.join(folder, base_filename + ".jpg") # Lu√¥n l∆∞u d∆∞·ªõi d·∫°ng JPG sau khi n√©n
    temp_save_path = os.path.join(folder, "temp_" + filename)

    try:
        print(f"      ‚Ü≥ ƒêang t·∫£i ·∫£nh b√¨a: {url}")
        response = requests.get(url, stream=True, timeout=20)
        response.raise_for_status()
        
        with open(temp_save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        print(f"      ‚úì T·∫£i xong, b·∫Øt ƒë·∫ßu n√©n...")
        compress_and_save_image(temp_save_path, final_save_path, target_mb=1.0)
        
    except requests.exceptions.RequestException as e:
        print(f"      ‚úó L·ªói khi t·∫£i ·∫£nh b√¨a: {e}")
        # D·ªçn d·∫πp file t·∫°m n·∫øu c√≥ l·ªói t·∫£i
        if os.path.exists(temp_save_path):
            os.remove(temp_save_path)
            
# --- C√°c h√†m kh√°c gi·ªØ nguy√™n t·ª´ tr∆∞·ªõc ---

def get_page_soup(url, wait_for_selector=None):
    try:
        driver.get(url)
        if wait_for_selector:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, wait_for_selector)))
        return BeautifulSoup(driver.page_source, 'html.parser')
    except Exception as e:
        print(f"    ‚úó L·ªói khi t·∫£i URL {url}: {e}")
        return None

def get_project_title_and_soup(url):
    soup = get_page_soup(url)
    if not soup: return "Untitled Project", None
    title_tag = soup.find('h1', id='firstHeading')
    title = title_tag.text.split(' - ')[0].strip() if title_tag else "Untitled Project"
    return re.sub(r'[\\/*?:"<>|]', '', title), soup

def get_illustration_pages(soup):
    links = {}
    if not soup: return {}
    for a in soup.select('a[title*="Illustrations"]'):
        vol_match = re.search(r'Volume\s*(\d+)', a.get('title', ''), re.IGNORECASE)
        if vol_match:
            links[f"Volume {vol_match.group(1)}"] = BASE_URL + a['href']
    return links

def extract_all_image_links(illustration_page_url):
    print(f"    üîé ƒêang ph√¢n t√≠ch trang: {illustration_page_url}")
    soup = get_page_soup(illustration_page_url, wait_for_selector="li.gallerybox")
    if soup is None: return []
    all_links = []
    for link_tag in soup.select('li.gallerybox .thumb a'):
        if not link_tag.has_attr('href'): continue
        image_soup = get_page_soup(BASE_URL + link_tag['href'])
        if not image_soup: continue
        full_image_link_tag = image_soup.select_one('div.fullImageLink a, #file a')
        if full_image_link_tag and full_image_link_tag.has_attr('href'):
            url = full_image_link_tag['href']
            all_links.append('https:' + url if url.startswith('//') else BASE_URL + url if url.startswith('/') else url)
    print(f"    ‚û§ Ho√†n t·∫•t. L·∫•y ƒë∆∞·ª£c {len(all_links)} link ·∫£nh.")
    return all_links

def save_links_to_file(folder, filename, links_by_volume):
    os.makedirs(folder, exist_ok=True)
    path = os.path.join(folder, filename)
    sort_key = lambda item: int(re.search(r'\d+', item[0]).group())
    total_links = 0
    with open(path, 'w', encoding='utf-8') as f:
        f.write(f"=== T·∫§T C·∫¢ LINK ·∫¢NH - {os.path.basename(filename).replace('.txt', '')} ===\n\n")
        for volume, links in sorted(links_by_volume.items(), key=sort_key):
            if links:
                f.write(f"--- {volume} ---\n")
                for link in links: f.write(link + '\n')
                f.write('\n')
                total_links += len(links)
    print(f"\n‚úÖ ƒê√£ l∆∞u danh s√°ch {total_links} link v√†o t·ªáp: {path}")

# === MAIN ===
try:
    print(f"üåê ƒêang truy c·∫≠p PJ: {PROJECT_URL}")
    title, soup = get_project_title_and_soup(PROJECT_URL)
    print(f"üìò T√™n PJ: {title}")

    illustration_pages = get_illustration_pages(soup)
    print(f"üìÑ T√¨m th·∫•y {len(illustration_pages)} trang minh h·ªça.")

    all_volume_links = {}
    if not illustration_pages:
        print("Kh√¥ng t√¨m th·∫•y trang minh h·ªça n√†o.")
    else:
        cover_download_folder = os.path.join(OUTPUT_FOLDER, "Covers_Compressed")
        for volume, url in sorted(illustration_pages.items(), key=lambda item: int(re.search(r'\d+', item[0]).group())):
            print(f"\nüì• B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {volume}...")
            all_links = extract_all_image_links(url)
            if all_links:
                all_volume_links[volume] = all_links
                cover_url = all_links[0]
                file_ext = os.path.splitext(cover_url.split('?')[0])[1] or '.jpg'
                safe_volume_name = re.sub(r'[\\/*?:"<>|]', '', volume)
                filename = f"{title} - {safe_volume_name}{file_ext}"
                download_image_and_compress(cover_url, cover_download_folder, filename)
            else:
                print(f"    - Kh√¥ng l·∫•y ƒë∆∞·ª£c link n√†o cho {volume}.")

    if all_volume_links:
        save_links_to_file(OUTPUT_FOLDER, f"{title} - All Links.txt", all_volume_links)
    else:
        print("\nKh√¥ng c√≥ link ·∫£nh n√†o ƒë∆∞·ª£c thu th·∫≠p.")
finally:
    if 'driver' in locals() and driver:
        driver.quit()
    print("\nHo√†n t·∫•t. Tr√¨nh duy·ªát ƒë√£ ƒë√≥ng.")